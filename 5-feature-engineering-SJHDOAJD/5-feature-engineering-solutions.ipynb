{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/ucl-logo.svg\" width=100%>\n",
    "</div>\n",
    "\n",
    "### UCL-ELEC0136 Data Acquisition and Processing Systems 2024\n",
    "University College London\n",
    "# Lab 5: Feature engineering\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "- Gain practical experience of dealing with real-world data\n",
    "- Develop intuition and understanding of how operations on input features can affect model performance, and when to use them\n",
    "- Develop familiarity different methods for assessing feature importance\n",
    "\n",
    "### Outline\n",
    "\n",
    "0. [Setup](#0-setup)\n",
    "1. [Dealing with missing values](#1-dealing-with-missing-values)\n",
    "2. [Transforming the distributions of features](#2-transforming-the-distributions-of-features)\n",
    "3. [Discretising an input feature](#3-discretising-an-input-feature)\n",
    "4. [Feature selection](#4-feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 0. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Prerequisites\n",
    "First, we need to install the necessary packages for this lab. In addition to what you have previously installed, the packages are:\n",
    "\n",
    "- `ucimlrepo`: real-life datasets for machine learning\n",
    "- `scikit-learn`: statistical machine learning models\n",
    "- `shap`: importance/relevancy metrics\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 0.1: Install prerequisites</h4>\n",
    "\n",
    "Install the packages that are required for this lab.\n",
    "\n",
    "<details>\n",
    "<summary>üîé Hint</summary>\n",
    "\n",
    "Remember, there are two steps to adding packages to your `daps` Python environment. You need to:\n",
    "1. **Add the package to the requirements file**.\n",
    "2. **Install all the requirements in the requirements file.**\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- Go to requirements.txt\n",
    "- Add:\n",
    "  ```\n",
    "  ucimlrepo\n",
    "  scikit-learn\n",
    "  ```\n",
    "- In terminal (Linux/Mac) or Anaconda prompt (Windows) navigate to the directory containing requirements.txt\n",
    "- Do `conda activate daps`\n",
    "- Do `pip install -r requirements.txt`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Fetching the dataset\n",
    "\n",
    "We'll be using a dataset containing data from a heart disease study in Cleveland, OH, USA.\n",
    "Using this data, we should be able to predict whether a patient has heart disease, given various measurements that can be taken by a doctor.\n",
    "\n",
    "For a more detailed explanation of the dataset, see [here](https://archive.ics.uci.edu/dataset/45/heart+disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import ssl\n",
    "\n",
    "# Ignore ssl certificate verification\n",
    "# We have to do this because in between writing the lab and delivering it, the SSL certificate of the UCIML website expired.\n",
    "# This is a hacky fix. It is terrible practice.\n",
    "# The correct thing to do would be to pester the website owners to update their SSL certificate, but we didn't have time.\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets \n",
    "  \n",
    "heart_disease.variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example inputs\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example outputs\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `num` is nonzero if the patient has some form of heart disease. We need to encode this to a binary variable (`0` or `1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 0.1: Encode output variable</h4>\n",
    "\n",
    "Modify the `y` DataFrame so that entries are `1` if the patient has some form of heart disease and `0` otherwise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y != 0\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the performance of a model, we need some data that the model hasn't seen. `scikit-learn` provides a handy function for producing a \"test\" (held-out) set, that is only used during evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we should preferably also use a *validation* set, but we're skipping it here for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 0.2: Create training and test sets</h4>\n",
    "\n",
    "Using `train_test_split` only, shuffle the `(X, y)` data and split it into a training and a test set, with 80% of the data in the training set.\n",
    "\n",
    "For reproducibility, create a variable, `seed`, and set it to `42`. Use this to set the `random_state` of `train_test_split`. We'll reuse this variable in future to reseed other random processes.\n",
    "\n",
    "Your variable names should be `(X_train, y_train)` and `(X_test, y_test)`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this fails, it means you did something wrong!\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Using a machine learning model\n",
    "\n",
    "In this lab, we'll be using a [*support vector machine*](https://scikit-learn.org/stable/modules/svm.html) classifier. This is a simple but powerful statistical learning model provided by `scikit-learn`. \n",
    "\n",
    "> Understanding the inner workings of this model is not the point of this lab, so we treat it as a black box.\n",
    " \n",
    "We'll demo the SVM here using a **different** dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from models import support_vector_machine\n",
    "\n",
    "# Load the iris dataset\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "# Train/test split\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, random_state=42)\n",
    "\n",
    "model = support_vector_machine.train(X_iris_train, y_iris_train)\n",
    "support_vector_machine.evaluate(model, X_iris_train, y_iris_train, X_iris_test, y_iris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the functions we're using: \n",
    "\n",
    "- `support_vector_machine.train` takes in the training data and returns a trained classifier\n",
    "- `support_vector_machine.evaluate` takes in the trained classifier, the training data, and the test data, and prints the score on the training and test sets.\n",
    "\n",
    "In the rest of the lab, you'll see how the performance of the classifier on the **heart disease dataset** can be affected by feature engineering. At each stage, we'll retrain the model on the feature-engineered data and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do when receiving data is to check whether there are any missing values.\n",
    "\n",
    "As `X_train` is a `pd.DataFrame`, we can use the `isna` (short for \"is not applicable\" or \"is not a number\") method to generate a table that has `0` where the values are present, and `1` if the values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "help(pd.DataFrame.isna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "- *Methods* are like functions that 'belong' to a specific object.\n",
    "- In the output of the `help` command above, there's an example of using the `isna` method of a `pd.DataFrame` object.\n",
    "- In `pandas`, you can often chain together methods on DataFrames in one line, like this:\n",
    "  ```python\n",
    "  X_train.sum().max()\n",
    "  ```\n",
    "- You might want to have the DataFrame documentation page open so that you can easily find any methods you need - this section should be solved using DataFrame methods alone (i.e., without manually iterating over rows in the data).\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 1.1: Compute the percentage of missing values</h4>\n",
    "\n",
    "Using methods of the `X_train` DataFrame, compute the percentage of missing values for each input feature.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values seem to only occur in two input features, and are missing in only a small amount of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 1.2: Deal with the missing values</h4>\n",
    "\n",
    "For each feature that has missing values:\n",
    "\n",
    "- Visualise the distribution of the values for that feature using a bar chart\n",
    "  - Remember to label your axes\n",
    "  - [Optional] If you like, you could also display bars for the NaN entries, which allows you to compare their frequency with the other categories\n",
    "- Select a method for dealing with the missing values, and **justify your answer in the Q/A box below**. Also explain why one *other* method would be **unsuitable**.\n",
    "- Implement your solution, and evaluate it using `evaluate`\n",
    "  - Your solution should modify `X_train`, `y_train`, `X_test`, and `y_test`\n",
    "  - Remember, **imputing missing values in the test set must only be done using information from the training set!**\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "- Create a copy of the unmodified dataframes so that if you make a mistake you don't have to re-run the notebook from the beginning!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 5]\n",
    "\n",
    "for col in [\"ca\", \"thal\"]:\n",
    "    # Get unique values\n",
    "    unique_values = X_train[col].unique()\n",
    "    # NaN is a unique value, so we replace it with something -ve so it's visible on the plot\n",
    "    # NOTE: This is optional, you can remove this line and the NaN values will be invisible\n",
    "    unique_values[-1] = -5\n",
    "    # Get counts\n",
    "    counts = X_train[col].value_counts() # Note that value_counts doesn't provide a count for NaNs, so we have to do it separately\n",
    "    nan_count = X_train[col].isna().sum() # NOTE: This is optional, depending on whether you wanted to visualise the NaNs\n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    plt.bar(unique_values, [*counts, nan_count]) # The notation of the second argument creates a single new list from a list and an int; if you haven't counted the NaNs, you can replace the entire second argument with just the non-NaN counts\n",
    "    plt.title(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- The instructions did not explain how to plot the bar chart, and the fact that you need the `value_counts` of the bins - students are expected to be able to research this themselves.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What method(s) will you use for dealing with the missing values? Why?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Valid points:\n",
    "- Missing data is a small fraction, so we could just discard it.\n",
    "- For each column with missing data, there is one categorical value that is the most common by a large margin (`0` for `ca`, `3` for `thal`). This means that we could probably use replacement by the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What is an unsuitable method for dealing with the missing values? Why?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** Valid points:\n",
    "- Missing data is a small fraction, so adding it as a separate output class would probably degrade performance.\n",
    "- Numerical mean / median etc not appropriate, as the data is categorical.\n",
    "- We haven't explored correlations yet, so it may be that all the NaNs are closely related. It could be argued that imputing with the mode would interfere with correlations between features, reducing performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Discard rows with missing values\n",
    "# Copy to avoid modifying (this is just so we can demonstrate both methods in the solutions)\n",
    "X_train_discard = X_train.copy()\n",
    "X_test_discard = X_test.copy()\n",
    "# Drop rows with missing values\n",
    "X_train_discard = X_train_discard.dropna()\n",
    "X_test_discard = X_test_discard.dropna()\n",
    "# Drop the corresponding rows in the outputs\n",
    "y_train_discard = y_train[y_train.index.isin(X_train_discard.index)]\n",
    "y_test_discard = y_test[y_test.index.isin(X_test_discard.index)]\n",
    "\n",
    "print(\"Result when discarding:\")\n",
    "model_discard = support_vector_machine.train(X_train_discard, y_train_discard)\n",
    "support_vector_machine.evaluate(model_discard, X_train_discard, y_train_discard, X_test_discard, y_test_discard)\n",
    "\n",
    "# Example 2: Replace NaNs with the mode\n",
    "# Copy to avoid modifying (this is just so we can demonstrate both methods in the solutions)\n",
    "X_train_mode = X_train.copy()\n",
    "X_test_mode = X_test.copy()\n",
    "for col in [\"ca\", \"thal\"]:\n",
    "    # Find the mode of the training data\n",
    "    mode = X_train_mode[col].mode()[0]\n",
    "    # Replace NaNs in the training set\n",
    "    X_train_mode[col] = X_train_mode[col].fillna(mode)\n",
    "    # Replace NaNs in the test set\n",
    "    X_test_mode[col] = X_test[col].fillna(mode)\n",
    "    \n",
    "print(\"Result when replacing with the mode:\")\n",
    "model_mode = support_vector_machine.train(X_train_mode, y_train)\n",
    "support_vector_machine.evaluate(model_mode, X_train_mode, y_train, X_test_mode, y_test)\n",
    "\n",
    "# We'll use discarding for the rest of the notebook\n",
    "X_train = X_train_discard\n",
    "X_test = X_test_discard\n",
    "y_train = y_train_discard\n",
    "y_test = y_test_discard\n",
    "\n",
    "# Uncomment this if you want to test with mode instead\n",
    "# X_train = X_train_mode\n",
    "# X_test = X_test_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- If discarding the NaNs, ensure that the values are discarded from all of `X_train`, `y_train`, `X_test` and `y_test`. Discarding the rows in y that have NaNs in X requires a bit more coding.\n",
    "- If replacing with the mode, ensure that NaNs in `X_test` are replaced with the mode from `X_train`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Transforming the distributions of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many statistical classifiers work best if the distribution of each input feature follows a Gaussian distribution.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 2.1: Visualising feature distributions</h4>\n",
    "\n",
    "- Using `matplotlib`, make a figure comprised of a subplot grid with 2 rows and 7 columns\n",
    "- On each subplot, plot the **histogram** of one of the input features\n",
    "- Which of the continuous variables appear to be **not** Gaussian-distributed? Write your answer in the Q/A box.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "- If your figure has overlapping labels or looks a bit squashed, try using `plt.tight_layout()` at the end of the cell.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(X_train.columns): \n",
    "    plt.subplot(2, 7, i+1)\n",
    "    plt.hist(X[col])\n",
    "    plt.title(col)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Which of the continuous variables are not Gaussian distributed?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** The continuous variables are `age`, `trestbs`, `chol`, `thalach`, and `oldpeak`. The non-Gaussian ones are `trestbs`, `chol`, `thalach`, and `oldpeak`. `age` looks fairly Gaussian, but the others are skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 2.2: Transforming the distributions </h4>\n",
    "\n",
    "- `scipy` provides an implementation of the Yeo-Johnson transformation. Find it (via your favourite search engine) and use it to transform the variables you identified above.\n",
    "- Evaluate the model on the transformed data, and compare to the scores on the untransformed data.\n",
    "- How have the scores of the model on the training and test set changed? Have we improved the model's ability to generalise? How do you know? Write your answer in the Q/A box.\n",
    "- Visualise histograms of the transformed distributions\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "- Create a copy of the unmodified dataframes so that if you make a mistake you don't have to re-run the notebook from the beginning.\n",
    "- Remember to re-train the model on the transformed data using `support_vector_machine.train`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import yeojohnson\n",
    "from models import support_vector_machine\n",
    "\n",
    "X_train_transformed = X_train.copy()\n",
    "X_test_transformed = X_test.copy()\n",
    "for col in [\"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]:\n",
    "    # yeojohnson returns the transformed data and the \"best\" (maximum likelihood) lambda value given the training data\n",
    "    # We reuse that lambda value in transforming the test data\n",
    "    X_train_transformed[col], l = yeojohnson(X_train_transformed[col])\n",
    "    X_test_transformed[col] = yeojohnson(X_test_transformed[col], l)\n",
    "\n",
    "print(\"Without transformation:\")\n",
    "base_model = support_vector_machine.train(X_train, y_train)\n",
    "support_vector_machine.evaluate(base_model, X_train, y_train, X_test, y_test)\n",
    "print(\"With transformation:\")\n",
    "transformed_model = support_vector_machine.train(X_train_transformed, y_train)\n",
    "support_vector_machine.evaluate(transformed_model, X_train_transformed, y_train, X_test_transformed, y_test)\n",
    "\n",
    "# Save the modification\n",
    "X_train = X_train_transformed\n",
    "X_test = X_test_transformed\n",
    "base_model = transformed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How have the scores of the model on the training and test set have changed? Have we improved the model's ability to generalise? How do you know?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Mean increased on both. Variance decreased on both. Hence, improved ability to generalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(X_train.columns): \n",
    "    plt.subplot(2, 7, i+1)\n",
    "    plt.hist(X_train[col])\n",
    "    plt.title(col)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Discretising an input feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we might want to turn a continuous feature into a discrete one. Sometimes, this is necessary, as some models only work with discrete inputs. Other times, we might find that discretisation improves the model's performance, as it helps it to generalise better.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 3.1: Discretising a continuous feature</h4>\n",
    "\n",
    "- The values in the transformed `thalach` column are high resolution and vary over a large range. This means that they might benefit from being discretised!\n",
    "- Plot a **cumulative density histogram** of the values in the `thalach` column of the training data.\n",
    "- Pandas provides `qcut`, a function for discretising a DataFrame based on quantiles (i.e., ensuring that each bin has a similar count). It also provides `cut`, a function for discretising based on predefined bin edges. Using these functions, discretise the `thalach` column into **10 bins**. The bin edges should be such that each bin has a similar count. Your final code should modify both training and test DataFrames.\n",
    "- Evaluate the performance of the model with a continuous `thalach` column and with a discretised `thalach` column.\n",
    "- Plot a bar chart of the binned `thalach` data.\n",
    "- In the Q/A box, explain why this discretisation has improved the model's performance.\n",
    "\n",
    "<details>\n",
    "    <summary>üîé Hint</summary>\n",
    "    You can plot a cumulative density histogram by supplying additional keyword arguments to the standard histogram function.\n",
    "    </details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "- Use data with no NaNs, and with Yeo-Johnson transformed variables.\n",
    "- Remember, you can't use statistical properties of the test set when transforming the test data points.\n",
    "- For the model we are using, the bin labels must be integers (for example, you could use `0` to represent `thalach < X`, `1` to represent `X < thalach < Y`, and so on).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_train[\"thalach\"], density=True, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binned = X_train.copy()\n",
    "X_test_binned = X_test.copy()\n",
    "nbins = 10\n",
    "# We use qcut to get bins with the same number of samples in each\n",
    "X_train_binned[\"thalach\"], bins = pd.qcut(X_train[\"thalach\"], q=nbins, retbins=True, labels=[i for i in range(nbins)])\n",
    "# We don't have access to the test data, so we can't use qcut here\n",
    "# Instead, we need to reuse the bins that were generated for the training data\n",
    "X_test_binned[\"thalach\"] = pd.cut(X_test[\"thalach\"], bins=bins, labels=[i for i in range(nbins)])\n",
    "\n",
    "print(\"Without binning:\")\n",
    "support_vector_machine.evaluate(base_model, X_train, y_train, X_test, y_test)\n",
    "print(\"With binning:\")\n",
    "binned_model = support_vector_machine.train(X_train_binned, y_train)\n",
    "support_vector_machine.evaluate(binned_model, X_train_binned, y_train, X_test_binned, y_test)\n",
    "\n",
    "X_train = X_train_binned\n",
    "X_test = X_test_binned\n",
    "base_model = binned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- You need to set the labels argument to be integers, otherwise the model will raise an error\n",
    "- You also need to make sure that `retbins` is set in `qcut` to get the bin values back, so that you can use them for the test data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've discretised the data, it can be plotted as a bar chart (discrete categories)\n",
    "plt.bar(X_train_binned[\"thalach\"].value_counts().index, X_train_binned[\"thalach\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- The instructions did not explain how to plot the bar chart, and the fact that you need the `value_counts` of the bins - students are expected to be able to research this themselves.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Why has discretising <code>thalach</code> improved the model's performance?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** In discretising `thalach`, we've grouped together patients with a similar `thalach` score. This allows us to **reduce the effect of noise** due to the excessively high precision in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, our goal is to reduce the number of inputs that the model requires to make a prediction, without losing too much performance. In fact, as we'll see, sometimes selecting only the most relevant features can even *boost* performance.\n",
    "\n",
    "We want to select **5** input features, from the 13 possible candidates. We explore a few methods for identifying the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Using mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` provides `mutual_info_classif` to approximate the mutual information criterion for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "help(mutual_info_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 4.1: Identifying important features using mutual information</h4>\n",
    "\n",
    "- Compute the mutual information criterion for the training data using `mutual_info_classif`. Set the `random_state` parameter to the `seed` variable you defined before.\n",
    "- Plot a bar chart of the results, where the x-axis labels are column names and the y-axis values are the mutual information between each column and the output variable.\n",
    "- In the Q/A box, write the column names of the 5 features with highest mutual information with the output.\n",
    "- Evaluate the model with all 13 features, and with only the top 5 mutual information features. **Has the performance of the model improved, stayed the same, or got worse? What does that tell us about the feature importance?** Answer in the Q/A box.\n",
    "- Why might a model improve when the number of input features is reduced? Answer in the Q/A box.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "<b>üíé Tip</b>\n",
    "\n",
    "- Use the model that has been trained on the data with no NaNs, Yeo-Johnson transformed variables, and binned `thalach`.\n",
    "- You can use `np.argsort(A)` to get the indices that would sort an array `A`. The indices can then be used to get a sorted version of something else.\n",
    "- If you get a warning from `sklearn`, replace `y_train` with `np.ravel(y_train)`. This simply flattens the `y_train` array, to ensure that it's 1D.\n",
    "- For this section, don't modify or overwrite any of the DataFrames! Create new copies, instead.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "# Calculate mutual information\n",
    "mi = mutual_info_classif(X_train, np.ravel(y_train), random_state=seed)\n",
    "\n",
    "# Sort\n",
    "sorted_indices = np.argsort(mi)[::-1] # We reverse so that it is in descending order\n",
    "\n",
    "# Plot\n",
    "plt.bar(X_train.columns[sorted_indices], mi[sorted_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What are the 5 columns with the highest mutual information with the output?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: `ca`, `cp`, `thal`, `exang`, `sex`. If you haven't seeded correctly, you may have a different answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cols = X_train.columns[sorted_indices][:5] # Whether you take the first 5 or the last 5 depends on whether you asked argsort to sort descending or ascending\n",
    "X_train_mutual_info = X_train[best_cols]\n",
    "X_test_mutual_info = X_test[best_cols]\n",
    "\n",
    "print(\"Accuracy with all features:\")\n",
    "support_vector_machine.evaluate(base_model, X_train, y_train, X_test, y_test)\n",
    "print(\"Accuracy with top 5 features:\")\n",
    "mutual_info_model = support_vector_machine.train(X_train_mutual_info, y_train)\n",
    "support_vector_machine.evaluate(mutual_info_model, X_train_mutual_info, y_train, X_test_mutual_info, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Has the model performance improved, stayed the same, or got worse? What does that tell us about the feature importance?</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Improved. Most of the information is encoded in the top 5 MI features - the other features are not very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Why might a model improve when the number of features is reduced?</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Some of the other features may have been adding noise, which means that the model's performance is degraded when they are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Using SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP is a Python package for computing *Shapley values*, which are an idea from game theory. The basic principle is to **allocate credit for a model‚Äôs output among its input features**. Hence, for a given output prediction, we can gain insight into **which input features** contributed to the prediction.\n",
    "\n",
    "SHAP is a hugely powerful tool, as you can apply it to any machine learning model (even LLMs!) to provide a greater degree of interpretability. However, not many people have heard of it. If you're interested, there's some really cool theory behind it [[paper](https://arxiv.org/abs/1705.07874)].\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 4.2: Identifying important features using Shapley additive values (SHAP)</h4>\n",
    "\n",
    "- Using `shap.utils.sample`, generate a subset of 100 points from the training set to act as 'background data'. Ideally, we'd use the full training set, but it gets quite slow to compute, so we use a subset for this lab.\n",
    "- Create a SHAP explainer object using `shap.KernelExplainer`, your background data, and the class probability prediction function from our model (stored in `model.predict_proba`).\n",
    "- Run the explainer on the test set, storing the result in a variable called `shap_values`.\n",
    "- Using `shap.plots.bar`, visualise the average SHAP absolute values. What are the top 5 features according to SHAP? Does SHAP agree with the mutual information criterion? Answer in the Q/A box.\n",
    "- Has the model performance improved, stayed the same, or got worse? What does that tell us about the feature importance?  Answer in the Q/A box.\n",
    "<details>\n",
    "<summary>üîé Hint</summary>\n",
    "If you're stuck, see whether you can find anything helpful on <a href=https://github.com/shap/shap#sample-notebooks>SHAP's Git repo</a>.\n",
    "</details>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "<b>üíé Tip</b>\n",
    "\n",
    "- Use the model that has been trained on the data with no NaNs, Yeo-Johnson transformed variables, and binned `thalach`.\n",
    "- **Remember to set the random state of the `sample()` function to the `seed` variable from before.**\n",
    "- Computing SHAP values takes a minute or so, so why not watch [this explanation](https://www.youtube.com/watch?v=MQ6fFDwjuco) of what's going on while your code is running.\n",
    "- The exact choice of SHAP explainer that you use depends on the model that you want to explain. As the model we're using (nonlinear support vector machine) can be formulated using kernels, we use a `KernelExplainer`. Don't worry if none of those words mean anything to you, it's not important for this course.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# Sample 100 datapoints as \"background data\"\n",
    "# Add your code here:\n",
    "X_background = shap.utils.sample(X_train, 100, random_state=seed)\n",
    "\n",
    "# Create a SHAP explainer\n",
    "# Add your code here:\n",
    "explainer = shap.KernelExplainer(base_model.predict_proba, X_background)\n",
    "# Evaluate the explainer on the test set\n",
    "# Add your code here:\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# shap_values is an array of shape (n_samples, n_features, n_outputs)\n",
    "# Confusingly, although we only have binary outcomes (1 or 0), SHAP still returns 2 outputs\n",
    "# The output at index 1 is the probability that the patient has heart disease\n",
    "shap_values = shap_values[:, :, 1]\n",
    "\n",
    "# Plot the SHAP values\n",
    "# Add your code here:\n",
    "shap.plots.bar(shap_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What are the top 5 features according to SHAP? Does SHAP agree with the mutual information criterion?</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** `age`, `thalach`, `thal`, `ca`, `cp`. Does not agree with mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shap = X_train.copy()\n",
    "X_test_shap = X_test.copy()\n",
    "\n",
    "features = [\"age\", \"thalach\", \"thal\", \"ca\", \"cp\"]\n",
    "X_train_shap = X_train_shap[features]\n",
    "X_test_shap = X_test_shap[features]\n",
    "\n",
    "shap_model = support_vector_machine.train(X_train_shap, y_train)\n",
    "\n",
    "print(\"Accuracy with all features:\")\n",
    "support_vector_machine.evaluate(base_model, X_train, y_train, X_test, y_test)\n",
    "print(\"Accuracy with top 5 SHAP features:\")\n",
    "support_vector_machine.evaluate(shap_model, X_train_shap, y_train, X_test_shap, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Has the model performance improved, stayed the same, or got worse? What does that tell us about the feature importance?</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Insignificantly worse / stayed the same. The rest of the features are less informative/important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>üë©‚Äçüíª [Optional] Task 4.2b: Explaining single predictions with SHAP waterfall plots</h4>\n",
    "\n",
    "SHAP can also be used to understand why the model predicted a particular output class for a particular input.\n",
    "\n",
    "Using `shap.plots.waterfall`, compare 3 candidates (2 from the same class, and one from the other class). Draw some comparisons between them. For each patient, what are the most significant features? How sure is the model of its predictions?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate in [1, 2, 3]:\n",
    "    shap.plots.waterfall(shap_values[candidate, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Q: What are some similarities and differences between the patients? What are the most significant features? How sure is the model of its predictions?</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Answers will depend on the patients the student has chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Using principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not limited to keeping the input features in the same geometrical space that they came in. In fact, there might be a way of projecting the data onto another co-ordinate system where their influence is easier to separate.\n",
    "\n",
    "One method for doing this is *principal component analysis* (PCA), a concept from linear algebra. PCA effectively involves computing the eigenvalues and eigenvectors of the input data, and then using these as the new co-ordinate basis. In pictures, that looks something like this:\n",
    "\n",
    "![PCA in 2D](figures/pca.png)\n",
    "\n",
    "By only keeping the eigenvectors corresponding to the largest eigenvalues, we end up with a reduced set of input coordinates that should explain most of the data.\n",
    "\n",
    "If you're interested in more detail, check out [this excellent tutorial](https://setosa.io/ev/principal-component-analysis/), which is the source of the above figure.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 4.3 Identifying important (transformed) features using PCA</h4>\n",
    "\n",
    "- Scikit-learn provides a PCA function. Find its documentation and use it to compute the first 5 principal components of the training data. Transform the training and test data with these components.\n",
    "- Compare the model performance with only the first 5 principal components and the whole data. How does the performance compare to the mutual information and SHAP methods? Answer in the Q/A box. \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_train, y_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "pca_model = support_vector_machine.train(X_train_pca, y_train)\n",
    "\n",
    "print(\"Accuracy with all components:\")\n",
    "support_vector_machine.evaluate(base_model, X_train, y_train, X_test, y_test)\n",
    "print(\"Accuracy with 5 principal components:\")\n",
    "support_vector_machine.evaluate(pca_model, X_train_pca, y_train, X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: How does the PCA performance compare to the mutual information and SHAP methods?</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Better than SHAP, worse than mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>üë©‚Äçüíª [Optional] Task 4.3b: Quantifying the effect of more principal components</h4>\n",
    "\n",
    "- With PCA, we can compute how well a given set of principal components represents the data.\n",
    "- Create a plot showing how the percentage of variance explained by the principal components changes as the number of components is increased from 0 to 13.\n",
    "\n",
    "<details>\n",
    "<summary>üîé Hint</summary>\n",
    "The PCA object has a property that might be useful in computing the explained variance. The easiest way of generating the plot is with a loop.\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(14)\n",
    "fraction_of_variance_explained = []\n",
    "for n in n_components:\n",
    "  pca = PCA(n_components=n)\n",
    "  pca.fit(X_train, y_train)\n",
    "  fraction_of_variance_explained.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.plot(n_components, fraction_of_variance_explained)\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Fraction of variance explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©‚Äçüíª Task 4.4: Reviewing the results</h4>\n",
    "\n",
    "- What are the strengths and weaknesses of the feature selection methods we've discussed in this section? Answer in the Q/A box below.\n",
    "- Your answer should include reference to the **real-world scenario** of this dataset.\n",
    "- Also consider how the methods are affected by any sources of **randomness**, and how they deal with **correlated features**.\n",
    "</div>\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "<b>üíé Tip</b>\n",
    "\n",
    "- You might have to carefully revisit the documentation for `mutual_info_classif` to spot how randomness plays a role in this method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What are the strengths and weaknesses of the feature selection methods we've discussed in this section?</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Some examples of relevant ideas:\n",
    "\n",
    "Mutual information:\n",
    "- Strengths:\n",
    "  - Simple\n",
    "  - Fast\n",
    "  - Works well\n",
    "- Weaknesses:\n",
    "  - Requires Monte Carlo (random-sample-based) estimation of the mutual information, as it cannot be analytically computed (hence having to set the random seed)\n",
    "  - Inter-feature correlation is not removed\n",
    "\n",
    "SHAP:\n",
    "- Strengths:\n",
    "  - Highly flexible, can be applied to any model\n",
    "  - Can provide \"explainability scores\" for single datapoints - extremely interpretable! This is a big bonus when dealing with health situations.\n",
    "  - Removes inter-feature correlation\n",
    "  - Can be completely deterministic, if run using all the training data\n",
    "- Weaknesses:\n",
    "  - Seems to perform less well for feature selection\n",
    "  - Slow\n",
    "\n",
    "PCA:\n",
    "- Strengths:\n",
    "  - Performs ok\n",
    "  - Fast\n",
    "  - Exact, deterministic\n",
    "- Weaknesses:\n",
    "  - Transforming the data means you lose interpretability - \"the first principal component\" means a lot less than \"patient's blood pressure\"\n",
    "  - Requires all the data to compute the principal components - ie we're not reducing the number of measurements that a doctor will have to take"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
